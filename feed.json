{
  "version": "https://jsonfeed.org/version/1",
  "title": "レンコン畑でつかまえて",
  "home_page_url": "https://www.jyuch.dev/",
  "feed_url": "https://www.jyuch.dev/feed.json",
  "description": null,
  "author": {
    "name": "jyuch"
  },
  "items": [
    {
      "id": "https://www.jyuch.dev/posts/2026/01-17-excel-read-files/",
      "url": "https://www.jyuch.dev/posts/2026/01-17-excel-read-files/",
      "title": "Databricksのread_files関数でもExcelファイルを読み込みたい",
      "content_html": "<h2>2月8日追記</h2>\n<p>現時点ではフリガナ問題は解決しています。</p>\n<p>また、より大きいファイルでも取り込めるようになっていました。\nですが、調子に乗って大きなサイズを放り込むと処理が終わらないので、その辺はいい感じにする必要があります。</p>\n<h2>注意事項</h2>\n<p>本検証は2026年1月17時点の内容で検証しています。\nリリース版は挙動が異なる場合があります。</p>\n<p>また、Databricks Free Editionで評価しています。\nそのため、商用版と挙動が異なる場合があります。</p>\n<h2>はじめに</h2>\n<p>Databricksの<code>read_files</code>関数はCSVやJSONフォーマットをいい感じに読み込んでくれる関数です。\nその<code>read_files</code>関数がExcelファイルの読み込みをサポートしました。</p>\n<p>ですが、Excelフォーマットをプログラムで扱う際のトラップに悩まされてきた生産性アプリケーション開発者も多いのではないでしょうか。</p>\n<p>そこで、今回はトラップになりそうな部分の評価を行ってみました。</p>\n<h2>Office Open XMLフォーマット</h2>\n<p>今回はExcelフォーマットのうち、Office Open XMLフォーマット（<code>xlsx</code>）を評価してみます。\nそのOOXMLフォーマットのうち、今回の検証で触れる内容について軽く解説をしていきます。</p>\n<p>OOXMLはXMLの名前がついている通り、主にzipで圧縮されたXMLファイルで構成されています。</p>\n<p>たとえば、以下のようなExcelファイルをzip展開すると、以下のような構造となっています。</p>\n<p><img src=\"https://www.jyuch.dev/img/2026/01-17-excel-read-files/example.xlsx.png\" alt=\"example.xlsxの見た目\"></p>\n<pre><code class=\"language-txt\">example.xlsx\n│\n│  [Content_Types].xml\n│\n├─docProps\n│      app.xml\n│      core.xml\n│\n├─xl\n│  │  sharedStrings.xml\n│  │  styles.xml\n│  │  workbook.xml\n│  │\n│  ├─printerSettings\n│  │      printerSettings1.bin\n│  │\n│  ├─theme\n│  │      theme1.xml\n│  │\n│  ├─worksheets\n│  │  │  sheet1.xml\n│  │  │\n│  │  └─_rels\n│  │          sheet1.xml.rels\n│  │\n│  └─_rels\n│          workbook.xml.rels\n│\n└─_rels\n        .rels\n</code></pre>\n<p>そのうち、内容を確認する際に見るのが以下の二つのファイルになります。</p>\n<h3><code>xl\\worksheets\\sheet1.xml</code></h3>\n<p><code>xl\\worksheets</code>以下にはシート毎にシートの内容が含まれているXMLファイルが格納されています。</p>\n<p>上記の例だと、こんな感じに内容になるはずです。</p>\n<pre><code class=\"language-xml\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;yes&quot;?&gt;\n&lt;worksheet xmlns=&quot;http://schemas.openxmlformats.org/spreadsheetml/2006/main&quot;\n    xmlns:r=&quot;http://schemas.openxmlformats.org/officeDocument/2006/relationships&quot;\n    xmlns:mc=&quot;http://schemas.openxmlformats.org/markup-compatibility/2006&quot;\n    mc:Ignorable=&quot;x14ac xr xr2 xr3&quot;\n    xmlns:x14ac=&quot;http://schemas.microsoft.com/office/spreadsheetml/2009/9/ac&quot;\n    xmlns:xr=&quot;http://schemas.microsoft.com/office/spreadsheetml/2014/revision&quot;\n    xmlns:xr2=&quot;http://schemas.microsoft.com/office/spreadsheetml/2015/revision2&quot;\n    xmlns:xr3=&quot;http://schemas.microsoft.com/office/spreadsheetml/2016/revision3&quot;\n    xr:uid=&quot;{00000000-0001-0000-0000-000000000000}&quot;&gt;\n    &lt;dimension ref=&quot;A1:B2&quot; /&gt;\n    &lt;sheetViews&gt;\n        &lt;sheetView tabSelected=&quot;1&quot; workbookViewId=&quot;0&quot; /&gt;\n    &lt;/sheetViews&gt;\n    &lt;sheetFormatPr defaultRowHeight=&quot;18.75&quot; /&gt;\n    &lt;sheetData&gt;\n        &lt;row r=&quot;1&quot; spans=&quot;1:2&quot;&gt;\n            &lt;c r=&quot;A1&quot; t=&quot;s&quot;&gt;\n                &lt;v&gt;0&lt;/v&gt;\n            &lt;/c&gt;\n        &lt;/row&gt;\n        &lt;row r=&quot;2&quot; spans=&quot;1:2&quot;&gt;\n            &lt;c r=&quot;B2&quot; t=&quot;s&quot;&gt;\n                &lt;v&gt;1&lt;/v&gt;\n            &lt;/c&gt;\n        &lt;/row&gt;\n    &lt;/sheetData&gt;\n    &lt;phoneticPr fontId=&quot;1&quot; /&gt;\n    &lt;pageMargins left=&quot;0.7&quot; right=&quot;0.7&quot; top=&quot;0.75&quot; bottom=&quot;0.75&quot; header=&quot;0.3&quot; footer=&quot;0.3&quot; /&gt;\n    &lt;pageSetup paperSize=&quot;9&quot; orientation=&quot;portrait&quot; r:id=&quot;rId1&quot; /&gt;\n&lt;/worksheet&gt;\n</code></pre>\n<p><code>A1</code>と<code>B2</code>セルにデータが含まれているのが分かりますね。\nですが、謎の数字<code>0</code>と<code>1</code>があるだけで、内容は特には記述されていません。</p>\n<p>テキスト情報はどこに行ったのでしょう？</p>\n<p>その答えはShared String Table（SST、<code>sharedStrings.xml</code>）の中にあります。</p>\n<h3><code>xl\\sharedStrings.xml</code></h3>\n<p>SSTはその名の通り、複数の場所から参照されるテキスト情報を格納するためテーブルです。</p>\n<p>上記の例だとシートはSSTのインデックス番号を参照していたことになります。</p>\n<pre><code class=\"language-xml\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;yes&quot;?&gt;\n&lt;sst xmlns=&quot;http://schemas.openxmlformats.org/spreadsheetml/2006/main&quot; count=&quot;2&quot; uniqueCount=&quot;2&quot;&gt;\n    &lt;si&gt;\n        &lt;t&gt;Hello Excel&lt;/t&gt;\n        &lt;phoneticPr fontId=&quot;1&quot; /&gt;\n    &lt;/si&gt;\n    &lt;si&gt;\n        &lt;t&gt;こんにちはExcel&lt;/t&gt;\n        &lt;phoneticPr fontId=&quot;1&quot; /&gt;\n    &lt;/si&gt;\n&lt;/sst&gt;\n</code></pre>\n<p>なお、文字情報は必ずSSTに格納しなければならない訳ではなく、Excelを出力するライブラリはシートのXMLに直接文字を格納する場合が多いです。</p>\n<h2>Excelファイルの読み込み</h2>\n<p>というわけで、Excelの深淵を覗く準備ができたところで<code>read_files</code>関数でファイルを読み込んでみましょう。</p>\n<h3>単純な例</h3>\n<p>以下のような単純なExcelファイルを用意します。</p>\n<p><img src=\"https://www.jyuch.dev/img/2026/01-17-excel-read-files/01_simple_files.xlsx.png\" alt=\"\"></p>\n<p>最低限のオプションだけ設定すると以下のように出力されます。</p>\n<pre><code class=\"language-sql\">SELECT\n  *\nFROM\n  read_files(\n    &quot;/Volumes/excel_read_files/default/files/01_simple_files.xlsx&quot;,\n    format =&gt; &quot;excel&quot;,\n    schemaEvolutionMode =&gt; &quot;none&quot;\n  )\n</code></pre>\n<p><img src=\"https://www.jyuch.dev/img/2026/01-17-excel-read-files/output_01.png\" alt=\"\"></p>\n<p>スキーマを推論してくれるので、日付や数字もそれぞれの型として認識してくれています。</p>\n<p>ですが、小数が少し怪しいですね。\n内部的にはこの形で格納されているので、<code>read_files</code>関数が悪いわけではなさそうです。</p>\n<pre><code class=\"language-xml\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;yes&quot;?&gt;\n&lt;worksheet&gt;\n    &lt;sheetData&gt;\n        &lt;row r=&quot;1&quot; spans=&quot;1:4&quot;&gt;\n            &lt;c r=&quot;C1&quot;&gt;\n                &lt;v&gt;1.1000000000000001&lt;/v&gt;\n            &lt;/c&gt;\n        &lt;/row&gt;\n        &lt;row r=&quot;2&quot; spans=&quot;1:4&quot;&gt;\n            &lt;c r=&quot;C2&quot;&gt;\n                &lt;v&gt;2.2000000000000002&lt;/v&gt;\n            &lt;/c&gt;\n        &lt;/row&gt;\n        &lt;row r=&quot;3&quot; spans=&quot;1:4&quot;&gt;\n            &lt;c r=&quot;C3&quot;&gt;\n                &lt;v&gt;3.3&lt;/v&gt;\n            &lt;/c&gt;\n        &lt;/row&gt;\n        &lt;row r=&quot;4&quot; spans=&quot;1:4&quot;&gt;\n            &lt;c r=&quot;C4&quot;&gt;\n                &lt;v&gt;4.4000000000000004&lt;/v&gt;\n            &lt;/c&gt;\n        &lt;/row&gt;\n        &lt;row r=&quot;5&quot; spans=&quot;1:4&quot;&gt;\n            &lt;c r=&quot;C5&quot;&gt;\n                &lt;v&gt;5.5&lt;/v&gt;\n            &lt;/c&gt;\n        &lt;/row&gt;\n    &lt;/sheetData&gt;\n&lt;/worksheet&gt;\n</code></pre>\n<p>型が既知なのであれば、あらかじめスキーマヒントを与えておいたほうが良いかもしれません。</p>\n<pre><code class=\"language-sql\">SELECT\n  *\nFROM\n  read_files(\n    &quot;/Volumes/excel_read_files/default/files/01_simple_files.xlsx&quot;,\n    format =&gt; &quot;excel&quot;,\n    schemaEvolutionMode =&gt; &quot;none&quot;,\n    schemaHints =&gt; &quot;_c2 decimal(5,1)&quot;\n  )\n</code></pre>\n<p><img src=\"https://www.jyuch.dev/img/2026/01-17-excel-read-files/output_02.png\" alt=\"\"></p>\n<p>ちなみに、<code>inferColumnTypes</code>はExcel読み込みでは考慮されないようです。</p>\n<pre><code class=\"language-sql\">SELECT\n  *\nFROM\n  read_files(\n    &quot;/Volumes/excel_read_files/default/files/01_simple_files.xlsx&quot;,\n    format =&gt; &quot;excel&quot;,\n    schemaEvolutionMode =&gt; &quot;none&quot;,\n    inferColumnTypes =&gt; false\n  )\n</code></pre>\n<p><img src=\"https://www.jyuch.dev/img/2026/01-17-excel-read-files/output_03.png\" alt=\"\"></p>\n<p>スキーマヒントを与えて強制的に文字列とすると、to_stringをしたような形式になります。</p>\n<p>また、日付は<code>MM/DD/YY</code>形式になります。USロケールなのかもしれません。</p>\n<pre><code class=\"language-sql\">SELECT\n  *\nFROM\n  read_files(\n    &quot;/Volumes/excel_read_files/default/files/01_simple_files.xlsx&quot;,\n    format =&gt; &quot;excel&quot;,\n    schemaEvolutionMode =&gt; &quot;none&quot;,\n    schemaHints =&gt; &quot;_c1 string, _c2 string, _c3 string&quot;\n  )\n</code></pre>\n<p><img src=\"https://www.jyuch.dev/img/2026/01-17-excel-read-files/output_04.png\" alt=\"\"></p>\n<h3>日本語を含む例</h3>\n<p>次はよくオフィスで流通していそうなファイルを読み込んでみます。</p>\n<p>今回の検証で初めて知ったのですが、Excelは変換時の元テキストをフリガナとして格納してくれるっぽいです。</p>\n<p><img src=\"https://www.jyuch.dev/img/2026/01-17-excel-read-files/02_%E6%97%A5%E6%9C%AC%E8%AA%9E%E3%82%92%E5%90%AB%E3%82%80%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB.xlsx.png\" alt=\"\"></p>\n<p>関係ないセルを読まないよう、<code>dataAddress</code>で読み込む場所を明示してあげます。</p>\n<pre><code class=\"language-sql\">SELECT\n  *\nFROM\n  read_files(\n    &quot;/Volumes/excel_read_files/default/files/02_日本語を含むファイル.xlsx&quot;,\n    dataAddress =&gt; &quot;'お供　給与表'!B2:C5&quot;,\n    headerRows =&gt; 1,\n    format =&gt; &quot;excel&quot;,\n    schemaEvolutionMode =&gt; &quot;none&quot;\n  )\n</code></pre>\n<p><img src=\"https://www.jyuch.dev/img/2026/01-17-excel-read-files/output_05.png\" alt=\"\"></p>\n<p><strong>良くないですね。</strong>\nフリガナも拾ってきてしまっています。</p>\n<p>少なくとも日本語だと意図してフリガナを拾ってほしいケースはあまりないと思うので、GAまでにフリガナを拾わないか、オプションで選択出来るようになるといいなと思います。</p>\n<p>ちなみに、この例だとSSTはこのようになっています。\nシート名として入力したテキストもフォネティックを拾っているとは思いませんでした。</p>\n<pre><code class=\"language-xml\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;yes&quot;?&gt;\n&lt;sst xmlns=&quot;http://schemas.openxmlformats.org/spreadsheetml/2006/main&quot; count=&quot;8&quot; uniqueCount=&quot;8&quot;&gt;\n    &lt;si&gt;\n        &lt;t&gt;お供&lt;/t&gt;\n        &lt;rPh sb=&quot;1&quot; eb=&quot;2&quot;&gt;\n            &lt;t&gt;トモ&lt;/t&gt;\n        &lt;/rPh&gt;\n        &lt;phoneticPr fontId=&quot;1&quot; /&gt;\n    &lt;/si&gt;\n    &lt;si&gt;\n        &lt;t&gt;きび団子支給数&lt;/t&gt;\n        &lt;rPh sb=&quot;2&quot; eb=&quot;4&quot;&gt;\n            &lt;t&gt;ダンゴ&lt;/t&gt;\n        &lt;/rPh&gt;\n        &lt;rPh sb=&quot;4&quot; eb=&quot;7&quot;&gt;\n            &lt;t&gt;シキュウスウ&lt;/t&gt;\n        &lt;/rPh&gt;\n        &lt;phoneticPr fontId=&quot;1&quot; /&gt;\n    &lt;/si&gt;\n    &lt;si&gt;\n        &lt;t&gt;猿&lt;/t&gt;\n        &lt;rPh sb=&quot;0&quot; eb=&quot;1&quot;&gt;\n            &lt;t&gt;サル&lt;/t&gt;\n        &lt;/rPh&gt;\n        &lt;phoneticPr fontId=&quot;1&quot; /&gt;\n    &lt;/si&gt;\n    &lt;si&gt;\n        &lt;t&gt;犬&lt;/t&gt;\n        &lt;rPh sb=&quot;0&quot; eb=&quot;1&quot;&gt;\n            &lt;t&gt;イヌ&lt;/t&gt;\n        &lt;/rPh&gt;\n        &lt;phoneticPr fontId=&quot;1&quot; /&gt;\n    &lt;/si&gt;\n    &lt;si&gt;\n        &lt;t&gt;雉&lt;/t&gt;\n        &lt;rPh sb=&quot;0&quot; eb=&quot;1&quot;&gt;\n            &lt;t&gt;キジ&lt;/t&gt;\n        &lt;/rPh&gt;\n        &lt;phoneticPr fontId=&quot;1&quot; /&gt;\n    &lt;/si&gt;\n    &lt;si&gt;\n        &lt;t&gt;令和8年1月17日改定&lt;/t&gt;\n        &lt;rPh sb=&quot;0&quot; eb=&quot;2&quot;&gt;\n            &lt;t&gt;レイワ&lt;/t&gt;\n        &lt;/rPh&gt;\n        &lt;rPh sb=&quot;3&quot; eb=&quot;4&quot;&gt;\n            &lt;t&gt;ネン&lt;/t&gt;\n        &lt;/rPh&gt;\n        &lt;rPh sb=&quot;5&quot; eb=&quot;6&quot;&gt;\n            &lt;t&gt;ガツ&lt;/t&gt;\n        &lt;/rPh&gt;\n        &lt;rPh sb=&quot;8&quot; eb=&quot;9&quot;&gt;\n            &lt;t&gt;ニチ&lt;/t&gt;\n        &lt;/rPh&gt;\n        &lt;rPh sb=&quot;9&quot; eb=&quot;11&quot;&gt;\n            &lt;t&gt;カイテイ&lt;/t&gt;\n        &lt;/rPh&gt;\n        &lt;phoneticPr fontId=&quot;1&quot; /&gt;\n    &lt;/si&gt;\n    &lt;si&gt;\n        &lt;t&gt;※変更時はおじいさんとおばあさんの承認を得ること&lt;/t&gt;\n        &lt;rPh sb=&quot;1&quot; eb=&quot;4&quot;&gt;\n            &lt;t&gt;ヘンコウジ&lt;/t&gt;\n        &lt;/rPh&gt;\n        &lt;rPh sb=&quot;17&quot; eb=&quot;19&quot;&gt;\n            &lt;t&gt;ショウニン&lt;/t&gt;\n        &lt;/rPh&gt;\n        &lt;rPh sb=&quot;20&quot; eb=&quot;21&quot;&gt;\n            &lt;t&gt;エ&lt;/t&gt;\n        &lt;/rPh&gt;\n        &lt;phoneticPr fontId=&quot;1&quot; /&gt;\n    &lt;/si&gt;\n    &lt;si&gt;\n        &lt;t&gt;全然関係ないデータ&lt;/t&gt;\n        &lt;rPh sb=&quot;0&quot; eb=&quot;4&quot;&gt;\n            &lt;t&gt;ゼンゼンカンケイ&lt;/t&gt;\n        &lt;/rPh&gt;\n        &lt;phoneticPr fontId=&quot;1&quot; /&gt;\n    &lt;/si&gt;\n&lt;/sst&gt;\n</code></pre>\n<p>手動でフリガナを消してあげると意図した感じに取り込んでくれます。</p>\n<p>ただし、手動で消すのは結構めんどくさいので、前処理でフリガナを消すプログラムを動かすとかでしょうか・・・</p>\n<pre><code class=\"language-sql\">SELECT\n  *\nFROM\n  read_files(\n    &quot;/Volumes/excel_read_files/default/files/03_日本語を含むファイル_フリガナ除去.xlsx&quot;,\n    dataAddress =&gt; &quot;'お供　給与表'!B2:C5&quot;,\n    headerRows =&gt; 1,\n    format =&gt; &quot;excel&quot;,\n    schemaEvolutionMode =&gt; &quot;none&quot;\n  )\n</code></pre>\n<p><img src=\"https://www.jyuch.dev/img/2026/01-17-excel-read-files/output_06.png\" alt=\"\"></p>\n<pre><code class=\"language-xml\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;yes&quot;?&gt;\n&lt;sst xmlns=&quot;http://schemas.openxmlformats.org/spreadsheetml/2006/main&quot; count=&quot;8&quot; uniqueCount=&quot;8&quot;&gt;\n    &lt;si&gt;\n        &lt;t&gt;令和8年1月17日改定&lt;/t&gt;\n        &lt;rPh sb=&quot;0&quot; eb=&quot;2&quot;&gt;\n            &lt;t&gt;レイワ&lt;/t&gt;\n        &lt;/rPh&gt;\n        &lt;rPh sb=&quot;3&quot; eb=&quot;4&quot;&gt;\n            &lt;t&gt;ネン&lt;/t&gt;\n        &lt;/rPh&gt;\n        &lt;rPh sb=&quot;5&quot; eb=&quot;6&quot;&gt;\n            &lt;t&gt;ガツ&lt;/t&gt;\n        &lt;/rPh&gt;\n        &lt;rPh sb=&quot;8&quot; eb=&quot;9&quot;&gt;\n            &lt;t&gt;ニチ&lt;/t&gt;\n        &lt;/rPh&gt;\n        &lt;rPh sb=&quot;9&quot; eb=&quot;11&quot;&gt;\n            &lt;t&gt;カイテイ&lt;/t&gt;\n        &lt;/rPh&gt;\n        &lt;phoneticPr fontId=&quot;1&quot; /&gt;\n    &lt;/si&gt;\n    &lt;si&gt;\n        &lt;t&gt;※変更時はおじいさんとおばあさんの承認を得ること&lt;/t&gt;\n        &lt;rPh sb=&quot;1&quot; eb=&quot;4&quot;&gt;\n            &lt;t&gt;ヘンコウジ&lt;/t&gt;\n        &lt;/rPh&gt;\n        &lt;rPh sb=&quot;17&quot; eb=&quot;19&quot;&gt;\n            &lt;t&gt;ショウニン&lt;/t&gt;\n        &lt;/rPh&gt;\n        &lt;rPh sb=&quot;20&quot; eb=&quot;21&quot;&gt;\n            &lt;t&gt;エ&lt;/t&gt;\n        &lt;/rPh&gt;\n        &lt;phoneticPr fontId=&quot;1&quot; /&gt;\n    &lt;/si&gt;\n    &lt;si&gt;\n        &lt;t&gt;全然関係ないデータ&lt;/t&gt;\n        &lt;rPh sb=&quot;0&quot; eb=&quot;4&quot;&gt;\n            &lt;t&gt;ゼンゼンカンケイ&lt;/t&gt;\n        &lt;/rPh&gt;\n        &lt;phoneticPr fontId=&quot;1&quot; /&gt;\n    &lt;/si&gt;\n    &lt;si&gt;\n        &lt;t&gt;お供&lt;/t&gt;\n        &lt;phoneticPr fontId=&quot;1&quot; /&gt;\n    &lt;/si&gt;\n    &lt;si&gt;\n        &lt;t&gt;きび団子支給数&lt;/t&gt;\n        &lt;phoneticPr fontId=&quot;1&quot; /&gt;\n    &lt;/si&gt;\n    &lt;si&gt;\n        &lt;t&gt;猿&lt;/t&gt;\n        &lt;phoneticPr fontId=&quot;1&quot; /&gt;\n    &lt;/si&gt;\n    &lt;si&gt;\n        &lt;t&gt;犬&lt;/t&gt;\n        &lt;phoneticPr fontId=&quot;1&quot; /&gt;\n    &lt;/si&gt;\n    &lt;si&gt;\n        &lt;t&gt;雉&lt;/t&gt;\n        &lt;phoneticPr fontId=&quot;1&quot; /&gt;\n    &lt;/si&gt;\n&lt;/sst&gt;\n</code></pre>\n<h3>計算式を含む例</h3>\n<p>次は式を含むExcelファイルを読み込ませてみます。</p>\n<p>ドキュメントページでは<a href=\"https://docs.databricks.com/aws/ja/query/formats/excel\">評価された数式を取り込みます</a>とありますが、どのような意味なのでしょうか。</p>\n<p><img src=\"https://www.jyuch.dev/img/2026/01-17-excel-read-files/04_formula.xlsx.png\" alt=\"\"></p>\n<pre><code class=\"language-sql\">SELECT\n  *\nFROM\n  read_files(\n    &quot;/Volumes/excel_read_files/default/files/04_formula.xlsx&quot;,\n    format =&gt; &quot;excel&quot;,\n    schemaEvolutionMode =&gt; &quot;none&quot;\n  )\n</code></pre>\n<p><img src=\"https://www.jyuch.dev/img/2026/01-17-excel-read-files/output_07.png\" alt=\"\"></p>\n<p>エクセルで数式セルを作成すると、内部的には数式と共に計算済みの値が格納されています。\nドキュメントの「評価された数式を取り込みます」とはおそらく内部的に保持されている計算済みの値を取り込むということを言っているのだと思われます。</p>\n<pre><code class=\"language-xml\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;yes&quot;?&gt;\n&lt;worksheet&gt;\n    &lt;sheetData&gt;\n        &lt;row r=&quot;1&quot; spans=&quot;1:3&quot;&gt;\n            &lt;c r=&quot;A1&quot;&gt;\n                &lt;v&gt;1&lt;/v&gt;\n            &lt;/c&gt;\n            &lt;c r=&quot;B1&quot;&gt;\n                &lt;v&gt;2&lt;/v&gt;\n            &lt;/c&gt;\n            &lt;c r=&quot;C1&quot;&gt;\n                &lt;f&gt;A1+B1&lt;/f&gt;\n                &lt;v&gt;3&lt;/v&gt;\n            &lt;/c&gt;\n        &lt;/row&gt;\n    &lt;/sheetData&gt;\n&lt;/worksheet&gt;\n</code></pre>\n<p>では、計算済みの値が格納されないケースがあるのでしょうか。</p>\n<p>全部のケースを確認した訳ではありませんが、ライブラリを使用してExcelファイルをプログラムから作成する場合は数式のみが格納されて計算済みの値が格納されない場合が多いです。</p>\n<p>たとえば、以下のようなコードを使用して上記のExcelと同じようなファイルを作成したとします。</p>\n<pre><code class=\"language-python\">import openpyxl\n\n\ndef main():\n    wb = openpyxl.Workbook()\n    sheet = wb.active\n\n    for it in range(1, 5):\n        sheet.cell(it, 1).value = it\n        sheet.cell(it, 2).value = it + 1\n        sheet.cell(it, 3).value = f&quot;=A{it}+B{it}&quot;\n\n    wb.save(&quot;05_formula_no_value.xlsx&quot;)\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</code></pre>\n<p>Excelで開くと未計算のセルは計算されて表示されますが、<code>read_files</code>関数では計算済みの値が無いため、正しく取得できていません。</p>\n<p><img src=\"https://www.jyuch.dev/img/2026/01-17-excel-read-files/05_formula_no_value.xlsx.png\" alt=\"\"></p>\n<pre><code class=\"language-sql\">SELECT\n  *\nFROM\n  read_files(\n    &quot;/Volumes/excel_read_files/default/files/05_formula_no_value.xlsx&quot;,\n    format =&gt; &quot;excel&quot;,\n    schemaEvolutionMode =&gt; &quot;none&quot;\n  )\n</code></pre>\n<p><img src=\"https://www.jyuch.dev/img/2026/01-17-excel-read-files/output_08.png\" alt=\"\"></p>\n<h3>複数のファイル</h3>\n<p>問題ないとは思いますが、念のため複数のファイルの読み込みを試してみます。</p>\n<p>以下のようなコードで100行×100ファイルのExcelを生成して読み込んでみます。</p>\n<pre><code class=\"language-python\">import random\nimport string\n\nimport openpyxl\n\n\ndef randomname(n):\n    return &quot;&quot;.join(random.choices(string.ascii_letters + string.digits, k=n))\n\n\ndef main():\n    for i in range(1, 101):\n        print(f&quot;06_excel_files\\\\book_{i:0&gt;3}.xlsx&quot;)\n        wb = openpyxl.Workbook()\n        sheet = wb.active\n\n        for r in range(1, 101):\n            sheet.cell(r, 1).value = i\n            sheet.cell(r, 2).value = r\n\n            for c in range(3, 6):\n                sheet.cell(r, c).value = randomname(10)\n\n            for c in range(6, 9):\n                sheet.cell(r, c).value = random.randint(0, 100)\n\n        wb.save(f&quot;06_excel_files\\\\book_{i:0&gt;3}.xlsx&quot;)\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</code></pre>\n<pre><code class=\"language-sql\">SELECT\n  *\nFROM\n  read_files(\n    &quot;/Volumes/excel_read_files/default/files/06_excel_files/&quot;,\n    format =&gt; &quot;excel&quot;,\n    schemaEvolutionMode =&gt; &quot;none&quot;\n  )\n</code></pre>\n<p><img src=\"https://www.jyuch.dev/img/2026/01-17-excel-read-files/output_09.png\" alt=\"\"></p>\n<p>良さそうですね。</p>\n<h3>サイズの大きいファイル</h3>\n<p>JavaでExcelを読み書きする代表的なライブラリと言えば<a href=\"https://poi.apache.org/\">Apache POI</a>があります。</p>\n<p>POIはUser API（Excelファイルをインメモリで展開するAPI）で巨大なファイルを扱おうとするとOOMでプログラムが爆散するという悲しき事故が発生しがちです。</p>\n<p>SparkはScala、すなわちJVMで動作するので同じ事故が発生するか否かは割と気になりますよね。</p>\n<p>以下のようなコードでそれぞれの行数を持つExcelファイルを作って、順番に読み込ませてみます。</p>\n<pre><code class=\"language-python\">import random\nimport string\n\nimport openpyxl\n\nROWS = [\n    10000,\n    10100,\n    10101,\n    10102,\n    10103,\n    10104,\n    10105,\n    10106,\n    10107,\n    10108,\n    10109,\n    10110,\n    10120,\n    10130,\n    10140,\n    10150,\n    10160,\n    10170,\n    10180,\n    10190,\n    10200,\n    10300,\n    10400,\n    10500,\n    11000,\n]\n\n\ndef randomname(n):\n    return &quot;&quot;.join(random.choices(string.ascii_letters + string.digits, k=n))\n\n\ndef main():\n    for row_count in ROWS:\n        print(f&quot;07_large_excel_files\\\\book_{row_count}.xlsx&quot;)\n        wb = openpyxl.Workbook()\n        sheet = wb.active\n\n        for r in range(1, row_count + 1):\n            sheet.cell(r, 1).value = row_count\n            sheet.cell(r, 2).value = r\n\n            for c in range(3, 6):\n                sheet.cell(r, c).value = randomname(10)\n\n            for c in range(6, 9):\n                sheet.cell(r, c).value = random.randint(0, 100)\n\n        wb.save(f&quot;07_large_excel_files\\\\book_{row_count}.xlsx&quot;)\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</code></pre>\n<p>詳細は省きますが、私の環境では10102行のファイルまでなら読み込んでくれましたが、10103行のファイルからは関数が返ってこなくなりました。</p>\n<p>関数の内部的な制限に引っかかったのか、それともDatabricks Free Editionのなんらかのレートリミットに引っかかったのかは分かりませんが、あまり大量の行数を有するファイルの読み込みには向かないようです。</p>\n<h2>おわりに</h2>\n<p>いくつか気になる点はありましたが、ベータ版の機能なのでGAまでには修正されると思います。</p>\n<p>Pythonでロジックを書かなくてもいい感じにExcelファイルが読み込めると、ファイルサーバ内に眠っているExcelファイルのデータの利活用のハードルも低くなると思います。</p>\n<p>GAが待ち遠しい機能ですね。</p>\n<p><a href=\"https://github.com/jyuch/excel-read-files\">jyuch / excel-read-files</a></p>\n<h2>余談</h2>\n<p><s>ファイルをドラッグアンドドロップでテーブルを作れる機能も内部的に<code>read_files</code>関数を使っているのか、現時点では同様に日本語のフリガナを拾ってしまいます。</s></p>\n<p>こちらも解決済みです。やったぜ</p>\n<p><img src=\"https://www.jyuch.dev/img/2026/01-17-excel-read-files/gui_ingestion.png\" alt=\"\"></p>\n<p>おわり</p>",
      "date_published": "2026-01-17T00:00:00Z"
    },
    {
      "id": "https://www.jyuch.dev/posts/2025/12-31-look-back-2025/",
      "url": "https://www.jyuch.dev/posts/2025/12-31-look-back-2025/",
      "title": "2025年の振り返りとか",
      "content_html": "<h2>はじめに</h2>\n<p>2025年の振り返りです。</p>\n<h2>Databricks</h2>\n<p>今年はお仕事でDatabricksをかなり触ってきました。</p>\n<p>特にDatabricksの環境を完全にゼロから（新規契約のAWSアカウントから）構築するのはあまりない経験だと思うので、珍しい経験が出来たなと思います。</p>\n<p>他にも基幹システムからのデータ取り込みやデータ加工パイプラインの構築・ガバナンス設計など<s>運用の全部を一人でやらさせられてる</s>大変貴重な経験を積ませていただいており大変うれしく思います。</p>\n<p>個人的にはDatabricksは少なくとも数年はデータとAI領域でのトップを走るプロダクトだと思っているので、しばらくはDababricksのお仕事を続けたいなとは思ってます。</p>\n<h2>生成AI</h2>\n<p>データ基盤を触っているとAI絡みのお話を頂くので、RAGや<a href=\"https://www.jyuch.dev/posts/2025/12-28-dspy-kasu-no-uso/\">プロンプト自動最適化</a>などのテクノロジはちょこちょこと触っていました。\n特にDSPyとMLflowはかなり注目しているので、2026年もこれらのプロダクトを追いかけたいと思っています。</p>\n<p>ただ、AIエージェントは技術的な領域としては面白いのですが、全社的なワークロードを動かしたときの性能面・コスト面の監視や基幹システムとの連接など考えないといけないことが多いなと現時点でも感じています。\n2026年はこの辺の本格的な開発・運用が始まりそうなので2026年はつらみポイントが高い年になりそうです。</p>\n<p>この辺を真面目にやろうとすると事業会社の内部でどうこう出来るレベルを超えそうと思っているのですが、皆さんどう考えてるんでしょうね？</p>\n<h2>資格とか</h2>\n<p>今年は<a href=\"https://www.jyuch.dev/posts/2025/07-13-2025-spring-nw/\">ネットワークスペシャリスト試験</a>と<a href=\"https://www.jyuch.dev/posts/2025/07-04-saa-c03/\">SAA</a>の2つを合格・取得できました。</p>\n<p>ネットワークスペシャリスト試験で勉強した内容は普通に日常の業務<s>やテキトーなことを吹っ掛けてくる自称有識者をボコすの</s>に役に立つので、ネットワークに触る人はみんな勉強すると良いと思います。</p>\n<p>IPAの試験体系が改定されて高度試験が消える前にデータベーススペシャリストもとっておこうかなと思ってます。\nセキュリティスペシャリストはどうなるんでしょうね？あれだけ体系が違うっぽいので残る気がしますが。</p>\n<p>AWS系の資格について特にコンプとかは考えていないのですが、SAPを持ってると貰えるお小遣いが増えるので受けてもいいかなと思っています。\nまぁ、弊社は全体的に資格難易度と金額が明らかに乖離しているのでにゃーん（社会性フィルタ）</p>\n<p>また、Databricks Certified Data Engineer Associateとかはあまり難しくなさそうなので試しに取ってみたいなとは思ってるのですが、受験料がｸｯｿ高いのと、弊社内でお小遣いが貰える資格ではないのでちょっと迷ってます。</p>\n<h2>コミュニティ</h2>\n<p>東京に勤務することになってから今年はDatabricksのユーザグループであるJEDAIのミートアップから始まり、結構な数の勉強会やもくもく会にお邪魔させていただきました。</p>\n<p>とくに<a href=\"https://mito-web-engineer.connpass.com/\">Ibaraki.dev</a>におかれましては、<a href=\"https://speakerdeck.com/jyuch/start-your-own-dns-forwarder-with-hickory-dns\">いきなりDNSの話をし始めたり誰も使ってないDNSのライブラリの話をし始めたり</a>と大変ご迷惑をお掛けしました。</p>\n<p>いろんな人と交流するとそれぞれの組織の考え方が分かったり、どんな組織でも大体似たようなことで悩んでるんだなというのが分かって楽しいですね。</p>\n<h2>おわりに</h2>\n<p>2026年も飛躍の年にしたいですね。</p>\n<p>おわり</p>",
      "date_published": "2025-12-31T00:00:00Z"
    },
    {
      "id": "https://www.jyuch.dev/posts/2025/12-28-dspy-kasu-no-uso/",
      "url": "https://www.jyuch.dev/posts/2025/12-28-dspy-kasu-no-uso/",
      "title": "ダウナー系生成AIに毎日カスの嘘を流し込まれたい",
      "content_html": "<h2>はじめに</h2>\n<p>最近は仕事でもAIエージェントの話を聞くことが多くなってきました。</p>\n<p>AIエージェントの性能はプロンプトの記述で大きく変わってきます。\nですが、プロンプトが少し変わったり、ナレッジカットオフが変わっただけで挙動が大きく変わるなんて話も聞きます。</p>\n<p>プロンプトの人力での調整はつらみポイントがとても高いので、個人的にはDSPyのようなプロンプト自動最適化に大きな期待を寄せています。</p>\n<p>そこで、今回は与えられたキーワードからいわゆる「カスの嘘」を生成するためのプロンプトをDSPyを使っての最適化を試してみます。</p>\n<h2>カスの嘘</h2>\n<p>まず、生成AIに生成させたいカスの嘘の要件を確認してみましょう。</p>\n<p>ここで生成したいカスの嘘とは以下のようなものです。</p>\n<ul>\n<li>雀が電線で感電しないのは、みんなで乗ることでダメージを分散しているからなんだよ</li>\n<li>地球儀は風水で最強なので、どこにいくつ置いてもいいんだよ</li>\n<li>一度開封したみりんを常温で保管すると、違法なんだって</li>\n</ul>\n<p>これらのカスの嘘の特徴をまとめてみると、</p>\n<ul>\n<li>一見して分かるようなあからさまな嘘ではないこと</li>\n<li>科学・文化・法律に依拠したうそであること</li>\n<li>「〇〇、〇〇」のフォーマットに従っており、20文字から40文字程度であること</li>\n<li>落ち着いた穏やかな口調であること</li>\n</ul>\n<p>であることが分かります。</p>\n<h2>DSPyを使用した最適化</h2>\n<p>手作業でのプロンプトの最適化では、上記に挙げた特徴に沿った出力を行うようにプロンプトを手作業で最適化する必要があります。</p>\n<p>DSPyでは代わりに生成AIを使用した処理をシグネチャとモジュールとして定義し、用意したデータセットと評価関数をを用いてスコアが高くなるようにプロンプトの最適化を行います。</p>\n<p>今回使用する<a href=\"https://arxiv.org/abs/2507.19457\">GEPA</a>では評価関数のメトリックだけでなく、評価関数が返すフィードバックテキストを元にプログラムの最適化を行います。</p>\n<p><img src=\"https://www.jyuch.dev/img/2025/12-21-dspy-kasu-no-uso/%E5%85%A8%E4%BD%93%E6%A7%8B%E6%88%90%E5%9B%B3.png\" alt=\"全体構成図\"></p>\n<h2>カスの嘘を吐くDSPyプログラム</h2>\n<p>今回は以下のようなシグネチャとモジュールを定義します。\nキーワードからカスを嘘を出力するとても単純なものです。</p>\n<p>また、DSPyではコメントドキュメントをプロンプトとして使用するのですが、自動プロンプト最適化を使用する場合は非常に非常にざっくりとしたものでもいい感じに最適化してくれます。</p>\n<pre><code class=\"language-python\">class LieGenerate(dspy.Signature):\n    &quot;&quot;&quot;与えられたキーワードからカスの嘘を生成します&quot;&quot;&quot;\n\n    keyword: str = dspy.InputField(desc=&quot;キーワード&quot;)\n    lie: str = dspy.OutputField(desc=&quot;生成されたカスの嘘&quot;)\n\n\nclass GenerateLie(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.extractor = dspy.ChainOfThought(LieGenerate)\n\n    def forward(self, keyword):\n        return self.extractor(keyword=keyword)\n</code></pre>\n<h2>評価関数</h2>\n<p>今回はカスの嘘を上記の観点で判定させるため、LLM as Judgeで評価を行います。</p>\n<p>生成AIは放っておくとファンタジーな嘘をつき始めるので、ファンタジー要素はあからさまなSF要素は減点するように評価をさせています。</p>\n<p>この評価用のプロンプトはある程度試行錯誤しないといけないんですよね・・・</p>\n<pre><code class=\"language-python\">def create_lie_metric(reflection_lm: dspy.LM):\n    class StyleEvaluation(dspy.Signature):\n        &quot;&quot;&quot;応答のスタイルを評価&quot;&quot;&quot;\n\n        response = dspy.InputField(desc=&quot;評価対象の応答&quot;)\n        criteria = dspy.InputField(desc=&quot;評価基準&quot;)\n        score = dspy.OutputField(desc=&quot;スコア（0-20）&quot;, format=int)\n        explanation = dspy.OutputField(desc=&quot;詳細な項目毎のスコアとその理由&quot;)\n\n    evaluator = dspy.ChainOfThought(StyleEvaluation)\n\n    def lie_metric(gold, pred, trace=None, pred_name=None, pred_trace=None):\n        criteria = &quot;&quot;&quot;\n        以下の基準で0-20点で評価してください:\n        1. キーワードに沿った嘘となっているか（2点）\n        2. 20-60文字で、「〇〇は、〇〇」や「〇〇はね、〇〇」という形式となっているか（2点）\n        3. あからさまな嘘ではなく、一見すると嘘とわからないようなもっともらしい嘘となっているか（6点）\n            - 魔法やドラゴン・妖精といった空想的存在に基づいた嘘ではないか\n            - 宇宙人や火星といった存在が確認されていないものや現在の科学水準で達成できない事柄を含んでいないか\n            - 食品が必修科目として学ばれるなど、キーワードと嘘の内容が乖離していないか\n            - 存在しない税が免除されるといった一つの文の中に複数の嘘が入っている場合は減点\n        4. 余計な解説や文が入っておらず、嘘のみの出力となっているか（2点）\n        5. 語頭に「ね、知ってる？」や「実は」を適切に使っているか（2点）\n        6. 語尾に「（な）んだよ」や「（な）んだって」、「らしいよ」を自然に接続して適切に使っているか（2点）\n        7. 日本語として自然で読みやすいか（2点）\n        8. 穏やかで落ち着いた口調か（2点）\n        &quot;&quot;&quot;\n        with dspy.context(lm=reflection_lm):\n            eval_result = evaluator(response=pred.lie, criteria=criteria)\n            score = min(20, max(0, float(eval_result.score))) / 20.0\n            explanation = eval_result.explanation\n            return dspy.Prediction(score=score, feedback=explanation)\n\n    return lie_metric\n</code></pre>\n<h2>最適化</h2>\n<p>今回はリフレクションLLMに<code>claude-sonnet-4-5-20250929</code>を使用し、タスクを実行するためのLLMとして以下のモデルを使って評価をします。</p>\n<ul>\n<li><code>claude-haiku-4-5-20251001</code></li>\n<li><code>gpt-oss-120b</code></li>\n<li><code>gpt-oss-20b</code></li>\n</ul>\n<p>本当は Claude Opus 4.5 を使いたかったのですが、トレーニングを1回通しで実行するだけで50ドルくらい消費しそうになったためビビッて Sonnet に変えたという経緯があります。</p>\n<p>学習1回あたり2時間30分程度、コストとしては10ドル程度掛かっていました。</p>\n<h2>生成出来るカスの嘘</h2>\n<p>各モデルがつく嘘は以下のような感じです。</p>\n<p>gpt-ossはやたら江戸時代の話題をしたがりますが、全体的にそれっぽい嘘をつけているのではないでしょうか。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">symbol</th>\n<th style=\"text-align:left\">keyword</th>\n<th style=\"text-align:left\">lie</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">gpt-oss-120b</td>\n<td style=\"text-align:left\">おみくじ</td>\n<td style=\"text-align:left\">実は、おみくじは江戸時代に寺の守護神が結果を予測したんだって</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">gpt-oss-20b</td>\n<td style=\"text-align:left\">おみくじ</td>\n<td style=\"text-align:left\">おみくじは、実は江戸時代に寺院で神の預言として引かれたものらしいよ</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">haiku-4-5</td>\n<td style=\"text-align:left\">おみくじ</td>\n<td style=\"text-align:left\">実は、おみくじは元々、神社の祭りの日に参加者の役割を決めるためのくじだったんだって</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">gpt-oss-120b</td>\n<td style=\"text-align:left\">お餅</td>\n<td style=\"text-align:left\">実は、お餅は江戸時代に、米ではなく大麦の粉を蒸して作っていたらしいよ</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">gpt-oss-20b</td>\n<td style=\"text-align:left\">お餅</td>\n<td style=\"text-align:left\">お餅は、実は江戸時代の神社で祭祀用の供え物として使われていたんだよ</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">haiku-4-5</td>\n<td style=\"text-align:left\">お餅</td>\n<td style=\"text-align:left\">実は、お餅は昔、神社の儀式で使う供え物として作られたのが始まりで、食べ物として広まったのは江戸時代らしいよ</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">gpt-oss-120b</td>\n<td style=\"text-align:left\">ちくわ</td>\n<td style=\"text-align:left\">ちくわは実は江戸時代に、魚のすり身ではなく、山形県の里山で取れた大根の根を乾燥させたものらしいよ</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">gpt-oss-20b</td>\n<td style=\"text-align:left\">ちくわ</td>\n<td style=\"text-align:left\">ちくわは、実は平安時代の貴族が魚の内臓を使い、皮を巻いて焼いたお菓子らしいよ</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">haiku-4-5</td>\n<td style=\"text-align:left\">ちくわ</td>\n<td style=\"text-align:left\">実は、ちくわは昔、竹の筒に詰めて蒸すことから『竹輪』という名前がついたんだって</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">gpt-oss-120b</td>\n<td style=\"text-align:left\">チョコレート</td>\n<td style=\"text-align:left\">チョコレートは実は19世紀末のスイスで、当初はカカオ豆の発酵を防ぐための保存料として開発されたらしいよ</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">gpt-oss-20b</td>\n<td style=\"text-align:left\">チョコレート</td>\n<td style=\"text-align:left\">チョコレートは、実は16世紀にスペインの商人が日本に持ち込んだ貴族用甘味料として使われていたんだよ</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">haiku-4-5</td>\n<td style=\"text-align:left\">チョコレート</td>\n<td style=\"text-align:left\">実は、チョコレートは昔、王族の儀式用の飲料として神聖視されていたんだって</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">gpt-oss-120b</td>\n<td style=\"text-align:left\">懐中電灯</td>\n<td style=\"text-align:left\">懐中電灯は実は、第一次世界大戦時にドイツ軍が潜水艦用に赤外線光源を内蔵していたらしいよ</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">gpt-oss-20b</td>\n<td style=\"text-align:left\">懐中電灯</td>\n<td style=\"text-align:left\">懐中電灯は、実は江戸時代に武士が夜の暗闇で作戦指示を確認するために携帯した小型の灯りで、取手に銀の刻印が付いていたんだよ</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">haiku-4-5</td>\n<td style=\"text-align:left\">懐中電灯</td>\n<td style=\"text-align:left\">実は、懐中電灯は元々、鉱山での作業中に両手を自由にするために懐に入れて使う照明として開発されたんだって</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">gpt-oss-120b</td>\n<td style=\"text-align:left\">乾電池</td>\n<td style=\"text-align:left\">実は、乾電池はかつて日本の昭和30年代に、アルカリ電解質ではなく銀酸化物が主成分だったらしいよ</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">gpt-oss-20b</td>\n<td style=\"text-align:left\">乾電池</td>\n<td style=\"text-align:left\">乾電池は、実は地下鉄の電力供給に使われていたと、当時の軍部秘書が記録していたんだよ</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">haiku-4-5</td>\n<td style=\"text-align:left\">乾電池</td>\n<td style=\"text-align:left\">実は、乾電池は元々、懐中電灯ではなく、医療用の刺激装置として開発されたんだって</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">gpt-oss-120b</td>\n<td style=\"text-align:left\">軽トラ</td>\n<td style=\"text-align:left\">実は、軽トラは昭和60年代に農業用から貨物輸送専用に改良されたんだって</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">gpt-oss-20b</td>\n<td style=\"text-align:left\">軽トラ</td>\n<td style=\"text-align:left\">軽トラは、実は昭和30年代に発明された、山間部の農家が手作りで改造した小型自動車の原型だって</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">haiku-4-5</td>\n<td style=\"text-align:left\">軽トラ</td>\n<td style=\"text-align:left\">実は、軽トラは元々、郵便配達員が使うために開発された専用車両なんだって</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">gpt-oss-120b</td>\n<td style=\"text-align:left\">初詣</td>\n<td style=\"text-align:left\">実は、初詣は江戸時代に商人が売り上げを祈願するために始めたらしいよ</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">gpt-oss-20b</td>\n<td style=\"text-align:left\">初詣</td>\n<td style=\"text-align:left\">初詣は、実は江戸時代から新年のご飯をお供えすると、その年の収穫が倍増すると信じられていたらしいよ</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">haiku-4-5</td>\n<td style=\"text-align:left\">初詣</td>\n<td style=\"text-align:left\">実は、初詣は江戸時代に商人たちが商売繁盛を祈るために始めた習慣なんだって</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">gpt-oss-120b</td>\n<td style=\"text-align:left\">生成AI</td>\n<td style=\"text-align:left\">生成AIは実は1970年代に日本の研究所で、音声合成だけでなく感情推定までできると宣伝されたらしいよ</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">gpt-oss-20b</td>\n<td style=\"text-align:left\">生成AI</td>\n<td style=\"text-align:left\">生成AIは、実は19世紀末のドイツ工学者がパターン生成装置として開発したらしいよ</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">haiku-4-5</td>\n<td style=\"text-align:left\">生成AI</td>\n<td style=\"text-align:left\">実は、生成AIって、元々は人間の脳の神経回路を模倣するために開発された技術なんだって</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">gpt-oss-120b</td>\n<td style=\"text-align:left\">東京タワー</td>\n<td style=\"text-align:left\">実は、東京タワーは当初、通信塔ではなく観測用の気象観測所として建てられたらしいよ</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">gpt-oss-20b</td>\n<td style=\"text-align:left\">東京タワー</td>\n<td style=\"text-align:left\">東京タワーは、実は昭和初期に東京郊外の灯台の設計図を活用して作られたらしいよ</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">haiku-4-5</td>\n<td style=\"text-align:left\">東京タワー</td>\n<td style=\"text-align:left\">実は、東京タワーは元々、戦後の電波障害を調査するための実験施設だったんだって</td>\n</tr>\n</tbody>\n</table>\n<h2>クロス評価</h2>\n<p>それぞれのモデルに最適なプロンプトが求められたので、プロンプトをそれぞれモデルに適用してスコアがどのようになるか評価してみました。</p>\n<p>どのモデルでもモデルに最適化されたプロンプトを使用したときが最も高いスコアを出していることが分かります。\nまた、gpt-oss-20bのようにパラメータ数が小さいモデルでは最適なプロンプトとそれ以外のプロンプトでのスコアの差が大きくなることが分かります。</p>\n<p><img src=\"https://www.jyuch.dev/img/2025/12-21-dspy-kasu-no-uso/cross-eval.png\" alt=\"クロス評価結果\"></p>\n<h2>一通り試してみての感想</h2>\n<p>ここからは一通り試してみての感想です。</p>\n<h3>学習データの準備が一番大変</h3>\n<p>この辺はよく言われると思います。\n今回はしょうもない嘘をつかせるという非常に単純なタスクだったので、学習データは30程度でも割と何とかなってます。\nですが、業務レベルの複雑さの場合、とにかく学習データを用意するのがとても大変になると思います。</p>\n<h3>評価関数のチューニングが大事</h3>\n<p>今回のタスクはキーワードから嘘をつくという、創作的なタスクを行わせています。\nそのため、与えられた入力からどれだけ期待する出力化という観点では評価がしづらく、LLM as Judgeで評価をさせています。</p>\n<p>評価関数で自分が欲しい嘘に対して高いスコアを返せるよういかに嘘の条件を言語化してLLM as Judgeで評価するのが大事でした。</p>\n<h3>APIの使用料金もそれなりに掛かる</h3>\n<p>GEPAはフィードバックを基にプロンプトを最適化しますが、そのプロンプトの最適化には出来るだけ高性能なLLMを使ったほうがいいらしいです。</p>\n<p>そんな高性能なLLMをすごい勢いで呼び出すので、当然APIの使用料金も高くなります。\nかといってプロンプトの最適化に安価なモデルを使用すると明らかにカスの嘘のクオリティも下がるので、この辺はクオリティと費用のバランスをうまくとる必要がありそうです。</p>\n<h3>試行錯誤が無くなるわけではない</h3>\n<p>LLM as Judgeを使う以上、ジャッジをさせるプロンプトそのものの最適化は人間がやらなくてはいけません。\n今回の例ですと、LLMはやたらとファンタジーな嘘をつきたがるので、それをいかにフィードバックで消すかというところに労力がかかっています。</p>\n<p>LLM as Judge自体をMIPROなりGEPAなりで最適化する手法もありますが、その辺の評価をどうするかや学習データの整備にも手間と時間が掛かるので、試行錯誤自体を消せるわけではないのかなという気がしています。</p>\n<h2>おわりに</h2>\n<p>生成AIにしょうもない嘘をつかせるのも安くはないですね・・・</p>\n<p><img src=\"https://www.jyuch.dev/img/2025/12-21-dspy-kasu-no-uso/%E3%82%B3%E3%82%B9%E3%83%88.png\" alt=\"カスの嘘をつかせるために28ドル掛かりました\"></p>\n<p><a href=\"https://github.com/jyuch/dspy-kasu-no-uso\">jyuch / dspy-kasu-no-uso</a></p>\n<p>おわり</p>\n<h2>参考資料</h2>\n<ul>\n<li><a href=\"https://github.com/mahm/dspy-demo\">mahm / dspy-demo</a></li>\n<li><a href=\"https://speakerdeck.com/masahiro_nishimi/dspy-meetup-tokyo-number-1-hazimetenodspy\">はじめてのDSPy - 株式会社ジェネラティブエージェント 西見公宏</a></li>\n</ul>",
      "date_published": "2025-12-28T00:00:00Z"
    },
    {
      "id": "https://www.jyuch.dev/posts/2025/10-26-improve-precision-using-dspy/",
      "url": "https://www.jyuch.dev/posts/2025/10-26-improve-precision-using-dspy/",
      "title": "DSPyを使用してLLMを使ったレシートの読み取り精度を向上させたい",
      "content_html": "<h2>はじめに</h2>\n<p>簡単な家計簿をつけているのですが、レシートの内容を確認して転記するのがめんどくさいなと感じていました。</p>\n<p>そこで、LLMを使って情報を抽出できないかなと考えていたのですが、せっかくなら<a href=\"https://dspy.ai/\">DSPy</a>を使って構造化出力するのと、プロンプト最適化を行ってみたいなということで試してみました。</p>\n<h2>シグネチャの準備</h2>\n<p>DSPyでは入力値と出力値をコードとして表現します。\nここではレシート画像から購入日と合計金額を抽出することにします。</p>\n<p>入力値と出力値はシグネチャとして表現され、<code>dspy.Signature</code>のサブクラスとして実装します。</p>\n<pre><code class=\"language-python\">from datetime import date\n\nimport dspy\n\n\nclass ExtractReceiptInfo(dspy.Signature):\n    &quot;&quot;&quot;Extract total amount from receipt image.&quot;&quot;&quot;\n\n    image: dspy.Image = dspy.InputField(desc=&quot;Receipt image.&quot;)\n    purchase_date: date = dspy.OutputField(desc=&quot;Purchase date of payment.&quot;)\n    total_amount: int = dspy.OutputField(desc=&quot;Total amount of payment.&quot;)\n\n\nclass ReceiptExtractor(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.extractor = dspy.ChainOfThought(ExtractReceiptInfo)\n\n    def forward(self, image):\n        return self.extractor(image=image)\n\n\ndef extraction_metric(gold, pred, trace=None):\n    metric = 0\n\n    if gold.total_amount == pred.total_amount:\n        metric += 1\n    if gold.purchase_date == pred.purchase_date:\n        metric += 1\n\n    if trace is None:\n        return metric / 2.0\n    else:\n        return metric == 2\n</code></pre>\n<h2>学習データの準備</h2>\n<p>DSPyでは教師データを使用して入力データから期待する出力をするようにプロンプトを最適化します。</p>\n<p>そのため、最初にある程度のレシート画像と、その画像からどのような結果を出力してほしいかの期待値をひたすら列挙する必要があります。</p>\n<p>ここでは、以下のような教師データを家中のあるだけのレシートを使って作成します。\nスキャナでレシートを読み込み、エクセルでレシート画像と期待値の組み合わせをひたすら入力します。</p>\n<p>このデータそのまま家計簿に突っ込めばよくね？とか考えてはいけません。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">image</th>\n<th style=\"text-align:right\">purchase_date</th>\n<th style=\"text-align:right\">total_amount</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">20251019_000.jpg</td>\n<td style=\"text-align:right\">2025-08-28</td>\n<td style=\"text-align:right\">162</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">20251019_001.jpg</td>\n<td style=\"text-align:right\">2025-08-19</td>\n<td style=\"text-align:right\">170</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">20251019_002.jpg</td>\n<td style=\"text-align:right\">2025-09-09</td>\n<td style=\"text-align:right\">162</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">...</td>\n<td style=\"text-align:right\">...</td>\n<td style=\"text-align:right\">...</td>\n</tr>\n</tbody>\n</table>\n<h2>LM Studioの準備</h2>\n<p>タスクを実行するLLMはLM Studioを使ってローカルで実行するので、LM Studioをインストールしておきます。\nモデルはGemma 3を使います。</p>\n<h2>Databricks Free Editionの準備</h2>\n<p>MIPROv2では教師として高性能なLLMを使用するのですが、ここではDatabricksでホストされているLlama 4 Maverickを利用します。</p>\n<p>Databricks Free Editionではレートリミットなどの制限はありますが、無償で使わせてくれるのでありがたく使います。Databricksさんはなんて太っ腹なんでしょう！（ステマその１）</p>\n<p>Settings → User → Developer → Access tokensからトークンを発行しておきます。\nまた、MLFlowも使いたいので、併せてExperimentsも作成しておきます。\nExperimentsを作成すると、外部からExperimentsを使うにはみたいな画面が表示されるので、表示された環境変数をコピーしておきます。</p>\n<p>最終的に以下の環境変数を登録します。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">環境変数</th>\n<th style=\"text-align:left\">例</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\"><code>DATABRICKS_API_BASE</code></td>\n<td style=\"text-align:left\"><code>https://dbc-12345678-abcd.cloud.databricks.com/serving-endpoints</code></td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><code>DATABRICKS_API_KEY</code></td>\n<td style=\"text-align:left\">Databricksのシークレット</td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><code>DATABRICKS_HOST</code></td>\n<td style=\"text-align:left\"><code>https://dbc-12345678-abcd.cloud.databricks.com</code></td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><code>DATABRICKS_TOKEN</code></td>\n<td style=\"text-align:left\">Databricksのシークレット</td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><code>MLFLOW_EXPERIMENT_ID</code></td>\n<td style=\"text-align:left\"><code>123456789012345</code></td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><code>MLFLOW_REGISTRY_URI</code></td>\n<td style=\"text-align:left\"><code>databricks-uc</code></td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><code>MLFLOW_TRACKING_URI</code></td>\n<td style=\"text-align:left\"><code>databricks</code></td>\n</tr>\n</tbody>\n</table>\n<p><code>DATABRICKS_API_BASE</code>と<code>DATABRICKS_API_KEY</code>はDatabricksのServing endpointsにアクセスする用で、それ以外はMLFlowにメトリックを送る用です。</p>\n<h2>トレーニング</h2>\n<p>必要なものがそろったらいよいよプロンプト最適化を実行します。\nここではとりあえずMIPROv2を使っていきます。</p>\n<p>余談ですが、DSPyはLLMにアクセスするために<a href=\"https://www.litellm.ai/\">LiteLLM</a>というライブラリを使用しているようです。</p>\n<p>LiteLLMではプレフィックスでどのプロバイダーのAPI形式（OpenAI互換やAnthropic互換など）を判断しているようです。\nまた、LM StudioはOpenAI互換です。</p>\n<p>そのため、OpenAI互換としてAPIを叩いてほしいのですが、<code>openai/google/gemma-3-12b</code>とかいう各方面から怒られそうなモデル名で指定をしないといけません。</p>\n<pre><code class=\"language-python\">import csv\nimport os\nfrom datetime import datetime\nfrom typing import List\n\nimport dspy\nimport mlflow\n\nfrom program import ReceiptExtractor, extraction_metric\n\nLMSTUDIO_API_BASE = os.environ[&quot;LMSTUDIO_API_BASE&quot;]\n\nteacher_llm = dspy.LM(\n    &quot;databricks/databricks-llama-4-maverick&quot;,\n    temperature=1.0,\n)\n\nstudent_llm = dspy.LM(\n    &quot;openai/google/gemma-3-12b&quot;,\n    api_base=LMSTUDIO_API_BASE,\n    api_key=&quot;dummy&quot;,\n)\n\n\ndef run_prompt_optimizer(train_examples: List[dspy.Example]):\n    student_program = ReceiptExtractor()\n    optimizer = dspy.MIPROv2(\n        metric=extraction_metric, prompt_model=teacher_llm, task_model=student_llm\n    )\n    compiled_program = optimizer.compile(student_program, trainset=train_examples)\n    compiled_program.save(&quot;./program.json&quot;, save_program=False)\n\n\ndef main():\n    mlflow.dspy.autolog(\n        log_compiles=True,\n        log_evals=True,\n        log_traces_from_compile=True,\n    )\n\n    dspy.configure(lm=student_llm)\n\n    train_examples: List[dspy.Example]\n    with open(&quot;./dataset/training.csv&quot;, encoding=&quot;utf_8&quot;) as f:\n        reader = csv.DictReader(f)\n        train_examples = [\n            dspy.Example(\n                image=dspy.Image.from_file(f&quot;./dataset/{row['image']}&quot;),\n                purchase_date=datetime.strptime(\n                    row[&quot;purchase_date&quot;], &quot;%Y-%m-%d&quot;\n                ).date(),\n                total_amount=int(row[&quot;total_amount&quot;]),\n            ).with_inputs(&quot;image&quot;)\n            for row in reader\n        ]\n\n    run_prompt_optimizer(train_examples)\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</code></pre>\n<h2>評価</h2>\n<p>学習が終わったら、とりあえずどのくらい違うのか評価してみましょう。</p>\n<p>ここでは、学習前と学習後、LLMのモデル、パラメータ数を変えてどのくらい差があるのかを確認しています。</p>\n<pre><code class=\"language-python\">import csv\nimport os\nfrom datetime import datetime\nfrom typing import List\n\nimport dspy\nfrom dspy.evaluate.evaluate import Evaluate\n\nfrom program import ReceiptExtractor, extraction_metric\n\nLMSTUDIO_API_BASE = os.environ[&quot;LMSTUDIO_API_BASE&quot;]\n\ngemma_3_12b = dspy.LM(\n    &quot;openai/google/gemma-3-12b&quot;,\n    api_base=LMSTUDIO_API_BASE,\n    api_key=&quot;dummy&quot;,\n)\n\ngemma_3_27b = dspy.LM(\n    &quot;openai/google/gemma-3-27b&quot;,\n    api_base=LMSTUDIO_API_BASE,\n    api_key=&quot;dummy&quot;,\n)\n\nllama_4_maverick = dspy.LM(\n    &quot;databricks/databricks-llama-4-maverick&quot;,\n)\n\n\ndef main():\n    dspy.configure(lm=gemma_3_12b)\n    original = ReceiptExtractor()\n    trained = ReceiptExtractor()\n    trained.load(&quot;./program.json&quot;)\n\n    train_examples: List[dspy.Example]\n    with open(&quot;./dataset/training.csv&quot;, encoding=&quot;utf_8&quot;) as f:\n        reader = csv.DictReader(f)\n        train_examples = [\n            dspy.Example(\n                image=dspy.Image.from_file(f&quot;./dataset/{row['image']}&quot;),\n                purchase_date=datetime.strptime(\n                    row[&quot;purchase_date&quot;], &quot;%Y-%m-%d&quot;\n                ).date(),\n                total_amount=int(row[&quot;total_amount&quot;]),\n            ).with_inputs(&quot;image&quot;)\n            for row in reader\n        ]\n\n    evaluate = Evaluate(\n        devset=train_examples, num_threads=1, display_progress=True, display_table=0\n    )\n\n    with dspy.context(lm=gemma_3_12b):\n        evaluate(original, metric=extraction_metric)\n        evaluate(trained, metric=extraction_metric)\n\n    with dspy.context(lm=gemma_3_27b):\n        evaluate(original, metric=extraction_metric)\n        evaluate(trained, metric=extraction_metric)\n\n    with dspy.context(lm=llama_4_maverick):\n        evaluate(original, metric=extraction_metric)\n        evaluate(trained, metric=extraction_metric)\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</code></pre>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">モデル</th>\n<th style=\"text-align:right\">最適化前</th>\n<th style=\"text-align:right\">最適化後</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\"><code>google/gemma-3-12</code></td>\n<td style=\"text-align:right\">84.0%</td>\n<td style=\"text-align:right\">96.2%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><code>google/gemma-3-27b</code></td>\n<td style=\"text-align:right\">80.2%</td>\n<td style=\"text-align:right\">97.2%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><code>databricks-llama-4-maverick</code></td>\n<td style=\"text-align:right\">100.0%</td>\n<td style=\"text-align:right\">100.0%</td>\n</tr>\n</tbody>\n</table>\n<p>あー、うん、まぁ、ねぇ。</p>\n<h2>おわりに</h2>\n<p>プロンプト最適化の部分については確かに最適化すれば性能は上がりましたが、最初から高性能なモデルを使えばそりゃ精度は高いよねというのを如実に見せつけられました。</p>\n<p>ですが、個人的には構造化出力をコードとして表現できるのはうれしみがありますね。\nプロンプトにJSON Schemaをくっつけて、出力をJSONパーサに食わせて正常にパース出来るのを祈るという作業から解放されるだけでもDSPyを使ううれしみがあると思います。</p>\n<p>教師データだけ用意しておけば、他のLLMが出てきたときは最適化と検証のループを回して、今までよりも成績が良ければ入れ替えるというサイクルをほぼ自動で回せます。</p>\n<p>ギョームでLLMを使う場合はこの辺のサイクルを回せるようにしておくと後々のつらみポイントを軽減出来ると思うので、みなさんDatabricksと合わせて使ってみてはいかがでしょうか。（ステマその２）</p>\n<p><a href=\"https://github.com/jyuch/extract-receipt/tree/master\">jyuch / extract-receipt</a></p>\n<p>おわり</p>",
      "date_published": "2025-10-26T00:00:00Z"
    },
    {
      "id": "https://www.jyuch.dev/posts/2025/10-12-pg18-uuidv7-is-monotonically-increasing/",
      "url": "https://www.jyuch.dev/posts/2025/10-12-pg18-uuidv7-is-monotonically-increasing/",
      "title": "PostgreSQL 18 の uuidv7 関数は単調性が保証されているから安心だねってお話",
      "content_html": "<h2>長いので先にまとめ</h2>\n<ul>\n<li>UUIDv7は<a href=\"https://datatracker.ietf.org/doc/html/rfc9562\">RFC9562</a>上ではミリ秒精度のタイムスタンプを持つことを要求している</li>\n<li>また、オプショナルとして<code>rand_a</code>及び<code>rand_b</code>の生成方法を工夫して単調増加を保証しても良いとされている</li>\n<li>PostgreSQL 18 の<code>uuidv7()</code>関数はRFC9562のSection 6.2 Method 3で提案されているタイムスタンプをサブミリ秒まで拡張する方法で単調増加を保証している\n<ul>\n<li>サブミリ秒レベルでタイムスタンプが衝突した場合は、最小精度でインクリメントすることでタイムスタンプの衝突を回避している</li>\n</ul>\n</li>\n</ul>\n<h2>はじめに</h2>\n<p>最近リリースされたPostgreSQL 18 で UUIDv7 の生成がサポートされました。</p>\n<p>個人的にはUUIDが生成順にソート可能になるうれしさがよく分からないのですが、いざ使うことになった際に困らないように調べてみることにしました。</p>\n<h2>UUIDv7</h2>\n<p>そもそも、UUIDv7はどのような構造をしているのでしょうか。</p>\n<p><a href=\"https://datatracker.ietf.org/doc/html/rfc9562#name-uuid-version-7\">5.7. UUID Version 7</a>からビットレイアウトを引用して確認してみましょう。</p>\n<pre><code class=\"language-txt\">    0                   1                   2                   3\n    0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n   |                           unix_ts_ms                          |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n   |          unix_ts_ms           |  ver  |       rand_a          |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n   |var|                        rand_b                             |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n   |                            rand_b                             |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n                   Figure 11: UUIDv7 Field and Bit Layout\n</code></pre>\n<p>バージョン（<code>ver</code>）とバリアント（<code>var</code>）を除くと、以下の要素から構成されていることが分かります。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">フィールド</th>\n<th style=\"text-align:left\">説明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\"><code>unix_ts_ms</code></td>\n<td style=\"text-align:left\">48ビットで表現されたミリ秒精度のUnixエポックタイムスタンプ</td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><code>rand_a</code></td>\n<td style=\"text-align:left\">12ビットのランダムフィールド</td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><code>rand_b</code></td>\n<td style=\"text-align:left\">62ビットのランダムフィールド</td>\n</tr>\n</tbody>\n</table>\n<p>RFCが最低限求めている仕様では、タイムスタンプはミリ秒精度となっています。\nそのため、同一ミリ秒内で複数回UUIDが生成された場合、生成された順序の並べ替えを保証出来なくなってしまいます。</p>\n<p>そのため、<a href=\"https://datatracker.ietf.org/doc/html/rfc9562#name-monotonicity-and-counters\">6.2. Monotonicity and Counters</a>では、高頻度でのUUIID生成環境下での単調増加性を保証するための方法が提案されています。</p>\n<h2>PostgreSQL 18 の UUIDv7</h2>\n<p>PostgreSQL 18 の<code>uuidv7()</code>関数はミリ秒タイムスタンプ + サブミリ秒タイムスタンプ + ランダム値で計算されていることが<a href=\"https://www.postgresql.org/docs/18/functions-uuid.html#FUNC_UUID_GEN_TABLE\">ドキュメントに記載されています。</a></p>\n<p>具体的には<code>uuid.c</code>の以下の<code>generate_uuidv7()</code>関数で実装されています。</p>\n<pre><code class=\"language-c\">/*\n * Generate UUID version 7 per RFC 9562, with the given timestamp.\n *\n * UUID version 7 consists of a Unix timestamp in milliseconds (48 bits) and\n * 74 random bits, excluding the required version and variant bits. To ensure\n * monotonicity in scenarios of high-frequency UUID generation, we employ the\n * method &quot;Replace Leftmost Random Bits with Increased Clock Precision (Method 3)&quot;,\n * described in the RFC. This method utilizes 12 bits from the &quot;rand_a&quot; bits\n * to store a 1/4096 (or 2^12) fraction of sub-millisecond precision.\n *\n * unix_ts_ms is a number of milliseconds since start of the UNIX epoch,\n * and sub_ms is a number of nanoseconds within millisecond. These values are\n * used for time-dependent bits of UUID.\n *\n * NB: all numbers here are unsigned, unix_ts_ms cannot be negative per RFC.\n */\nstatic pg_uuid_t *\ngenerate_uuidv7(uint64 unix_ts_ms, uint32 sub_ms)\n{\n\tpg_uuid_t  *uuid = palloc(UUID_LEN);\n\tuint32\t\tincreased_clock_precision;\n\n\t/* Fill in time part */\n\tuuid-&gt;data[0] = (unsigned char) (unix_ts_ms &gt;&gt; 40);\n\tuuid-&gt;data[1] = (unsigned char) (unix_ts_ms &gt;&gt; 32);\n\tuuid-&gt;data[2] = (unsigned char) (unix_ts_ms &gt;&gt; 24);\n\tuuid-&gt;data[3] = (unsigned char) (unix_ts_ms &gt;&gt; 16);\n\tuuid-&gt;data[4] = (unsigned char) (unix_ts_ms &gt;&gt; 8);\n\tuuid-&gt;data[5] = (unsigned char) unix_ts_ms;\n\n\t/*\n\t * sub-millisecond timestamp fraction (SUBMS_BITS bits, not\n\t * SUBMS_MINIMAL_STEP_BITS)\n\t */\n\tincreased_clock_precision = (sub_ms * (1 &lt;&lt; SUBMS_BITS)) / NS_PER_MS;\n\n\t/* Fill the increased clock precision to &quot;rand_a&quot; bits */\n\tuuid-&gt;data[6] = (unsigned char) (increased_clock_precision &gt;&gt; 8);\n\tuuid-&gt;data[7] = (unsigned char) (increased_clock_precision);\n\n\t/* fill everything after the increased clock precision with random bytes */\n\tif (!pg_strong_random(&amp;uuid-&gt;data[8], UUID_LEN - 8))\n\t\tereport(ERROR,\n\t\t\t\t(errcode(ERRCODE_INTERNAL_ERROR),\n\t\t\t\t errmsg(&quot;could not generate random values&quot;)));\n\n#if SUBMS_MINIMAL_STEP_BITS == 10\n\n\t/*\n\t * On systems that have only 10 bits of sub-ms precision,  2 least\n\t * significant are dependent on other time-specific bits, and they do not\n\t * contribute to uniqueness. To make these bit random we mix in two bits\n\t * from CSPRNG. SUBMS_MINIMAL_STEP is chosen so that we still guarantee\n\t * monotonicity despite altering these bits.\n\t */\n\tuuid-&gt;data[7] = uuid-&gt;data[7] ^ (uuid-&gt;data[8] &gt;&gt; 6);\n#endif\n\n\t/*\n\t * Set magic numbers for a &quot;version 7&quot; (pseudorandom) UUID and variant,\n\t * see https://www.rfc-editor.org/rfc/rfc9562#name-version-field\n\t */\n\tuuid_set_version(uuid, 7);\n\n\treturn uuid;\n}\n</code></pre>\n<p>詳しくは実装を読んでもらえればと思いますが、<code>rand_a</code>フィールドの12bitにナノ秒（Linux）を埋め込むか、マイクロ秒 + ランダム値（Windows・mac）を埋め込んでいます。</p>\n<p>また、現在時刻を供給している<code>get_real_time_ns_ascending()</code>関数内で、前回との時刻の差分が<code>rand_a</code>フィールドに埋め込むタイムスタンプの精度以下の場合は、最小精度（Linuxの場合は245マイクロ秒）を加算することでサブミリ秒内でタイムスタンプが衝突することを防いでいます。</p>\n<pre><code class=\"language-c\">/*\n * Get the current timestamp with nanosecond precision for UUID generation.\n * The returned timestamp is ensured to be at least SUBMS_MINIMAL_STEP greater\n * than the previous returned timestamp (on this backend).\n */\nstatic inline int64\nget_real_time_ns_ascending()\n{\n\tstatic int64 previous_ns = 0;\n\tint64\t\tns;\n\n\t/* Get the current real timestamp */\n\n#ifdef\t_MSC_VER\n\tstruct timeval tmp;\n\n\tgettimeofday(&amp;tmp, NULL);\n\tns = tmp.tv_sec * NS_PER_S + tmp.tv_usec * NS_PER_US;\n#else\n\tstruct timespec tmp;\n\n\t/*\n\t * We don't use gettimeofday(), instead use clock_gettime() with\n\t * CLOCK_REALTIME where available in order to get a high-precision\n\t * (nanoseconds) real timestamp.\n\t *\n\t * Note while a timestamp returned by clock_gettime() with CLOCK_REALTIME\n\t * is nanosecond-precision on most Unix-like platforms, on some platforms\n\t * such as macOS it's restricted to microsecond-precision.\n\t */\n\tclock_gettime(CLOCK_REALTIME, &amp;tmp);\n\tns = tmp.tv_sec * NS_PER_S + tmp.tv_nsec;\n#endif\n\n\t/* Guarantee the minimal step advancement of the timestamp */\n\tif (previous_ns + SUBMS_MINIMAL_STEP_NS &gt;= ns)\n\t\tns = previous_ns + SUBMS_MINIMAL_STEP_NS;\n\tprevious_ns = ns;\n\n\treturn ns;\n}\n</code></pre>\n<p>そのため、実用的かどうかと言われるとｱﾚですが、<code>uuidv7()</code>関数で生成されたUUIDv7からサブミリ秒のタイムスタンプを抽出することが出来ます。</p>\n<pre><code class=\"language-rust\">// サブミリ秒が12ビットで供給されている環境用\nfn uuidv7_to_timestamp(uuidv7: &amp;str) -&gt; (i64, i64) {\n    let uuid = uuidv7.replace(&quot;-&quot;, &quot;&quot;);\n    let uuid = u128::from_str_radix(&amp;*uuid, 16).unwrap();\n    let ms = (uuid &gt;&gt; 80) as i64;\n\n    let uuid = uuid.to_be_bytes();\n    let increased_clock_precision = (((uuid[6] &amp; 0x0fu8) as i64) &lt;&lt; 8) | uuid[7] as i64;\n    let ns = increased_clock_precision * SUBMS_MINIMAL_STEP_NS;\n\n    (ms, ns)\n}\n</code></pre>\n<p><a href=\"https://github.com/jyuch/pg-uuidv7-to-timestamp\">jyuch/pg-uuidv7-to-timestamp</a></p>\n<h2>PostgreSQL 18 でのUUIDv7生成例</h2>\n<p>最後に実際に<code>uuidv7()</code>関数を使ってUUIDv7を生成して、単調増加しているかを確認してみましょう。</p>\n<p>以下のようなテーブルを作成したうえで、</p>\n<pre><code class=\"language-sql\">create table uuidv7_test_table\n(\n    i     int,\n    clock timestamp,\n    value uuid\n);\n</code></pre>\n<p>以下のクエリで全力でUUIDv7を生成します。</p>\n<pre><code class=\"language-sql\">begin transaction;\ndo\n$do$\n  begin\n    for i in 1..1000000\n      loop\n        insert into uuidv7_test_table(i, clock, value) \n        values (i, clock_timestamp(), uuidv7());\n      end loop;\n    end\n$do$;\nend;\n</code></pre>\n<p>すると、以下のような結果となります。\n上記の<code>uuidv7_to_timestamp()</code>の結果を併記しています。</p>\n<pre><code class=\"language-sql\">select i, value, uuid_extract_timestamp(value), clock\nfrom uuidv7_test_table\norder by value;\n</code></pre>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">i</th>\n<th style=\"text-align:left\">value</th>\n<th style=\"text-align:left\">uuid_extract_timestamp</th>\n<th style=\"text-align:left\">clock</th>\n<th style=\"text-align:left\">uuidv7_to_timestamp</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">1</td>\n<td style=\"text-align:left\">0199d67d-81a7-713d-aed5-6bbd22a476c2</td>\n<td style=\"text-align:left\">2025-10-12 03:36:13.479000 +00:00</td>\n<td style=\"text-align:left\">2025-10-12 03:36:13.479071</td>\n<td style=\"text-align:left\">2025-10-12 03:36:13.479 UTC 77665</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">2</td>\n<td style=\"text-align:left\">0199d67d-81a7-7be4-ba18-b1d95ac5835f</td>\n<td style=\"text-align:left\">2025-10-12 03:36:13.479000 +00:00</td>\n<td style=\"text-align:left\">2025-10-12 03:36:13.479741</td>\n<td style=\"text-align:left\">2025-10-12 03:36:13.479 UTC 745780</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">3</td>\n<td style=\"text-align:left\">0199d67d-81a7-7c31-9ca1-df81b59c2e69</td>\n<td style=\"text-align:left\">2025-10-12 03:36:13.479000 +00:00</td>\n<td style=\"text-align:left\">2025-10-12 03:36:13.479762</td>\n<td style=\"text-align:left\">2025-10-12 03:36:13.479 UTC 764645</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">4</td>\n<td style=\"text-align:left\">0199d67d-81a7-7c45-b183-8ebf38326e24</td>\n<td style=\"text-align:left\">2025-10-12 03:36:13.479000 +00:00</td>\n<td style=\"text-align:left\">2025-10-12 03:36:13.479766</td>\n<td style=\"text-align:left\">2025-10-12 03:36:13.479 UTC 769545</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">5</td>\n<td style=\"text-align:left\">0199d67d-81a7-7c55-893a-c1685851f5f7</td>\n<td style=\"text-align:left\">2025-10-12 03:36:13.479000 +00:00</td>\n<td style=\"text-align:left\">2025-10-12 03:36:13.479770</td>\n<td style=\"text-align:left\">2025-10-12 03:36:13.479 UTC 773465</td>\n</tr>\n</tbody>\n</table>\n<p><code>uuid_extract_timestamp</code>関数がミリ秒までの精度しか返していませんが、仕様上はミリ秒精度があれば良いとされていることと、外部で生成されたUUIDv7でも対応できるようにこのようになっています。</p>\n<p>おわり</p>",
      "date_published": "2025-10-12T00:00:00Z"
    },
    {
      "id": "https://www.jyuch.dev/posts/2025/10-11-build-postgresql-18/",
      "url": "https://www.jyuch.dev/posts/2025/10-11-build-postgresql-18/",
      "title": "PostgreSQL 18 をソースコードからビルドしたい",
      "content_html": "<h2>はじめに</h2>\n<p>PostgreSQL 18 がリリースされたので、ソースコードからビルドする手順を確認してみました。</p>\n<p>まぁ、基本的には公式リファレンスと過去の自分のブログをなぞっているだけです。</p>\n<p>LLVMのサポートと、lz4・zstd圧縮のサポートを有効にしてビルドします。</p>\n<p><a href=\"https://www.postgresql.org/docs/18/installation.html\">Chapter 17. Installation from Source Code</a></p>\n<p><a href=\"https://www.jyuch.dev/posts/2021/build-pg-on-linux/\">PostgreSQLを野良ビルドしてローカルインストールしたい</a></p>\n<h2>環境</h2>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">ソフトウェア</th>\n<th style=\"text-align:left\">バージョン</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">OS</td>\n<td style=\"text-align:left\">Ubuntu 24.04.3 LTS</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">PostgreSQL</td>\n<td style=\"text-align:left\">18.0</td>\n</tr>\n</tbody>\n</table>\n<p>また、ソースコードは<code>$HOME/src</code>に展開するものとし、バイナリは<code>$HOME/.local/pg18.0</code>にインストールするものとします。</p>\n<h2>コンパイラ・ライブラリのインストール</h2>\n<p>ビルドするだけなら以下のパッケージを入れるだけでOKです。</p>\n<pre><code class=\"language-sh\">sudo apt install \\\n  build-essential \\\n  flex \\\n  bison \\\n  libreadline-dev \\\n  zlib1g-dev \\\n  liblz4-dev \\\n  libzstd-dev \\\n  llvm-20 \\\n  clang-20\n</code></pre>\n<p>ドキュメントを含めたフルビルドが必要なら以下のパッケージも必要になります。</p>\n<pre><code class=\"language-sh\">sudo apt install \\\n  docbook-xml \\\n  docbook-xsl \\\n  libxml2-utils \\\n  xsltproc \\\n  fop\n</code></pre>\n<h2>ソースコードのダウンロード・展開</h2>\n<p>以下のコマンドよりソースコードをダウロード・展開します。</p>\n<pre><code class=\"language-sh\">cd $HOME/src\ncurl -OL https://ftp.postgresql.org/pub/source/v18.0/postgresql-18.0.tar.gz\ntar zxvf postgresql-18.0.tar.gz\n</code></pre>\n<h2>ビルド・インストール</h2>\n<p>以下のコマンドより、<code>configure</code>を流します。</p>\n<pre><code class=\"language-sh\">cd postgresql-18.0\nmkdir build_temp &amp;&amp; cd build_temp\n$HOME/src/postgresql-18.0/configure \\\n  --prefix=$HOME/.local/pg18.0 \\\n  --with-icu \\\n  --with-lz4 \\\n  --with-zstd \\\n  --with-llvm \\\n  LLVM_CONFIG='/usr/bin/llvm-config-20' \\\n  CLANG='/usr/bin/clang-20' \\\n  CC='/usr/bin/clang-20' \\\n  CXX='/usr/bin/clang-20'\n</code></pre>\n<p>バイナリ系をビルドするだけなら以下のコマンドを実行します。</p>\n<pre><code class=\"language-sh\">make world-bin\nmake install-world-bin\n</code></pre>\n<p>ドキュメントを含めてすべてビルドするなら以下のコマンドを実行します。</p>\n<pre><code class=\"language-sh\">make world\nmake install-world\n</code></pre>\n<p>おわり</p>",
      "date_published": "2025-10-11T00:00:00Z"
    },
    {
      "id": "https://www.jyuch.dev/posts/2025/09-27-do-gen-ais-dream-of-biz/",
      "url": "https://www.jyuch.dev/posts/2025/09-27-do-gen-ais-dream-of-biz/",
      "title": "生成AIは業務の夢を見るか",
      "content_html": "<h2>はじめに</h2>\n<p>最近は猫も杓子も生成AIの話題で持ちきりです。\nソフトウェアエンジニアリングの領域では生成AIにコードを書かせる具体的な方法論が積極的に議論されています。</p>\n<p>対して、生成AIを実業務に載せる話となると、何とも言えない抽象論のお話がメインで、生成AIに購買業務をやらせていますという話は聞きません。</p>\n<p>そこで、業務アプリを模したMCPを実装し、生成AIがそのMCPを介して人間の指示に従って業務を遂行できるのかを試してみました。</p>\n<h2>実装したMCP</h2>\n<p><a href=\"https://github.com/jyuch/badcompany-enterprise-mcp\">badcompany-enterprise-mcp</a></p>\n<p>今回は簡略化した購買業務を遂行するためのMCPを実装しました。</p>\n<p>MCPは以下のツールを備えており、生成AIが在庫数を取得したり発注を行えるようになっています。</p>\n<table>\n<thead>\n<tr>\n<th>ツール名</th>\n<th>説明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>query_stock</code></td>\n<td>指定したアイテムの在庫数を取得します。</td>\n</tr>\n<tr>\n<td><code>query_item_required</code></td>\n<td>製造対象アイテムに必要な部品とその数量を取得します。</td>\n</tr>\n<tr>\n<td><code>order_item</code></td>\n<td>指定したアイテムを発注します。</td>\n</tr>\n<tr>\n<td><code>query_pending_orders</code></td>\n<td>発注残のリストを取得します。</td>\n</tr>\n<tr>\n<td><code>query_item_info</code></td>\n<td>アイテムの基本情報（名前・種別）を取得します。</td>\n</tr>\n</tbody>\n</table>\n<p>余談ですが、このコードは混じりっけなしの純度100%の手書きコードです。\nまぁ、MCPとか書いてみたかったし・・・</p>\n<h2>実行</h2>\n<p>今回は<code>gpt-oss-120b</code>をLM Studioで動かし、ClineにMCPを接続し動作させてみました。</p>\n<blockquote>\n<p>rules.mdを読んで以下の指示に従いなさい。\nPRODUCT-Aが10個必要です。\n足りない分を製造するためのアイテムを発注しなさい。</p>\n</blockquote>\n<p>PRODUCT-Aは在庫に1個あり、製造するのにPART-aが2個とPART-bが3個必要です。\nPART-aは在庫が1個、PART-bは在庫が2個あります。</p>\n<p>そのため、最終的にPART-aを17個とPART-bを25個発注する<code>order_item</code>を呼び出せれば成功となります。</p>\n<blockquote>\n<p>PRODUCT‑A を 10 個確保するために、在庫が不足していた部品 PART‑a と PART‑b の必要数量（それぞれ 18 個・27 個）を確認し、現在の在庫数（PART‑a が 1 個、PART‑b が 2 個）から不足分を計算しました。その結果、以下の発注を行いました。</p>\n<ul>\n<li>PART‑a を 17 個 発注（成功）</li>\n<li>PART‑b を 25 個 発注（成功）</li>\n</ul>\n<p>これにより、PRODUCT‑A の残り 9 個を製造するために必要な部品はすべて確保されました。現在在庫は以下の通りです。</p>\n<ul>\n<li>PRODUCT‑A : 1 個（既存）</li>\n<li>PART‑a : 18 個（発注分含む）</li>\n<li>PART‑b : 27 個（発注分含む）</li>\n</ul>\n<p>以上で、要求された「PRODUCT‑A が 10 個必要」というタスクは完了しました。</p>\n</blockquote>\n<h2>おわりに</h2>\n<p>とりあえず単純化された業務をMCPを介して実行することは出来ました。</p>\n<p>ただ、実業務に適用しようとすると、どうなんでしょうね？\nまぁ、考える機会があれば考えてみます。</p>\n<p>おわり</p>",
      "date_published": "2025-09-27T00:00:00Z"
    },
    {
      "id": "https://www.jyuch.dev/posts/2025/08-29-ucd/",
      "url": "https://www.jyuch.dev/posts/2025/08-29-ucd/",
      "title": "Unicode Character Database の歩き方",
      "content_html": "<h2>はじめに</h2>\n<p>所用でUnicode Character Database（UCD）で調べ物をする必要があったのですが、Unicodeのサイトのググラビリティが恐ろしく低く調べるのに難儀したので、未来の自分のために残しておこうというｱﾚです。</p>\n<h2>Unicode® Standard</h2>\n<p><a href=\"https://www.unicode.org/standard/standard.html\">Unicode® Standard</a>はUnicodeの仕様についてのトップページです。</p>\n<p>最新のUnicodeの仕様は<a href=\"https://www.unicode.org/versions/latest/\">https://www.unicode.org/versions/latest/</a>から飛べます。</p>\n<h2>UAX #41: Common References for Unicode Standard Annexes</h2>\n<p><a href=\"https://www.unicode.org/reports/tr41/\">UAX #41: Common References for UAXs</a>は他のUAXへのリファレンスセットです。</p>\n<h2>UAX #44: Unicode Character Database</h2>\n<p><a href=\"https://www.unicode.org/reports/tr44/\">UAX #44: Unicode Character Database</a>で各テキストファイルの中身が解説されています。</p>\n<p>また、<a href=\"https://www.unicode.org/Public/\">https://www.unicode.org/Public/</a>から各UnicodeバージョンのUCDテキストファイルをダウンロードできます。</p>\n<h2>Unicode Utilities</h2>\n<p><a href=\"https://util.unicode.org/UnicodeJsps/index.jsp\">Unicode Utilities</a>はUCDの内容を確認するのに役に立ちます。</p>\n<p>特に<a href=\"https://util.unicode.org/UnicodeJsps/character.jsp\">Unicode Utilities: Character Properties</a>は文字のプロパティを確認する際に特に便利です。</p>\n<p>デフォルトで表示されているのが☕なのは、こんなページを確認するような人は正常ではなく、多量のカフェインを要しているということを暗示しているのでしょうか？</p>\n<p>おわり</p>",
      "date_published": "2025-08-29T00:00:00Z"
    },
    {
      "id": "https://www.jyuch.dev/posts/2025/07-13-2025-spring-nw/",
      "url": "https://www.jyuch.dev/posts/2025/07-13-2025-spring-nw/",
      "title": "ネットワークスペシャリスト試験に合格したお話",
      "content_html": "<h2>はじめに</h2>\n<p>去年の12月の<a href=\"https://www.jyuch.dev/posts/2024/12-28-2024-autumn-ap/\">応用情報技術者試験</a>に続きネットワークスペシャリスト試験に合格したのでそのお話です。</p>\n<p>特に言うこともないのですが、せっかく合格したので何か書いておこうというｱﾚです。</p>\n<p><s>こいついっつも試験受けてんな</s></p>\n<p><img src=\"https://www.jyuch.dev/img/2025/07-05-2025-spring-nw/result.png\" alt=\"\"></p>\n<h2>会場とか雰囲気とか</h2>\n<p>前回に引き続きつくばの国際会議場で受験しました。</p>\n<p>国際会議場でのネットワークスペシャリスト試験は２つ部屋に分かれていたので正確なところはわかりませんが、少なくとも私が受験したほうの部屋では午前Ⅰを受験されている人は居ませんでした。\nあの部屋の広さで午前Ⅰを自分一人で受験すると心が折れるかもしれません。</p>\n<p>年齢層はというと応用情報と比べると若干年齢層が上がり、20代後半から30代の人が一番多かったと思います。\nちらほら学生っぽい方もいて、学生時代はこのような試験を受けようとすら思わなかった私からするとすごいなと素直に感心してました。</p>\n<p>私はというと、会場で参考書を見てもそわそわして全く集中できないタイプなので、午前Ⅱが始まるまでイヤホンで音楽を聴きながらひたすらラムネをボリボリ食っている異常者と化していました。</p>\n<h2>アドバイス？</h2>\n<p>午後Ⅰ・Ⅱはいくつかある問題から解く問題を選択する形式ですが、悠長に悩んでいる暇はないのである程度スパっと決める必要があります。</p>\n<p>そこで、私は問題選択の方針として不得手な分野を優先して回避するというのをあらかじめ決めていました。</p>\n<p>私の得意不得意の傾向は証明書 ＞ ルーティング ＞ セキュリティ ＞ マルチキャスト ＞ VoIP ＞＞＞ 無線という順なので、なるべく無線は避けるといった具合です。</p>\n<p>また、個人的にはこういう試験を暗記で乗り切るのはちょっと違うかなと思っていますが、どのみち午後Ⅱとかいう暗記ではどうあがいても乗り切れない問題を解かないといけない事を考えると、少しでも午後問題の対策に勉強時間を割いたほうがいいと思います。\nそのうえで午前Ⅱを安定感をもって突破するなら暗記してしまうのも手だと思います。</p>\n<p>また、午後Ⅱ問題も過去数年分は何周かはしたほうがいいと思います。\nこれは問題と解答を覚えるというよりかは、問題文の日本語に慣れるという意味合いと、〇〇文字で述べよという記述に慣れるという意味合いがあります。</p>\n<p>個人的には過去問は午前Ⅱは10分、午後Ⅰは30分、午後Ⅱは60分位で解けていたので、細かい時間しか取れない平日は午前Ⅱや午後Ⅰ、まとまった時間が取れる休日に午後Ⅱを解くようにすると時間を有効に使えると思います。</p>\n<p>まぁ、結局は午後問題は自分が解きやすい問題が出ることを祈ることしかできないので、最終的には祈祷力が問われます。</p>\n<h2>試験の感想</h2>\n<h3>午前Ⅱ</h3>\n<p>全体的な問題構成は例年通りといったところで、ざっくりと前半に計算問題、真ん中あたりにネットワークの知識を問う問題、後ろのほうに一般的なIT知識を問う問題構成となっていました。</p>\n<p>計算問題は割と過去問からの出題が多かった気がします。\nですので、ある程度過去問をやりこんでいて計算問題の答えの数字を覚えている人は計算せずに答えられたのかなと思います。\n特に答えが80になる例のアーランの問題は出題間隔的に出てもおかしくなかったので、予想が見事に的中して嬉しかった（？）です。</p>\n<p>ただ、今回出ると予想していたOSコマンドインジェクションが出題されなかったので、次回は出ると思います。</p>\n<h3>午後Ⅰ</h3>\n<ol>\n<li><strong>ルータの更新（BGPとOSPF、VRRP）</strong></li>\n<li>ネットワークの改善（NATとHTTP/2・HTTP/3・QUIC）</li>\n<li><strong>セキュアWebゲートウェイ（パケットフィルタリングとルーティング、証明書）</strong></li>\n</ol>\n<p>午後Ⅰは１と３を選びました。\nというよりかは、２の図を見たときに「AP」という文字が飛び込んできて、とっさに「無線やんけ」と早とちりして回避した結果１と３になったという感じです。</p>\n<p>解いた感想としては１と３はよくある午後Ⅰ問題です。</p>\n<p>特にBGPとOSPFは近年よく出てきていたので、今年も出てくると予想していました。\nただ、こうも頻出されると来年は傾向を変えてくるんじゃないかと疑いたくもなりますね。</p>\n<h3>午後Ⅱ</h3>\n<ol>\n<li><strong>社内ネットワークのIPv6対応（IPv4・IPv6のデュアルスタック）</strong></li>\n<li>IoTシステム（LPWA・CoAP・DTLS）</li>\n</ol>\n<p>ついにIPv6が出題されてしまいました。</p>\n<p>正直今年出てくるとは思っていなかったのですが、IPv6なんか良さそうだな程度の理由で<a href=\"https://www.lambdanote.com/collections/ipv6\">ラムダノートさんのプロフェッショナルIPv6</a>を眺めていた時期があったので何とかなりました。</p>\n<p>内容の難易度としてはグローバルユニキャストアドレスとリンクローカルアドレスがありますよとか、ルーターアドバタイズメントでプレフィックスを広告してSLAACでGUAを決定しますよという内容で、そこまでマニアックな内容ではないと思います。</p>\n<p>「IPv6アドレスは〇〇ビットごとに：で区切る」の〇〇を答えよみたいな設問の下にIPv6アドレスの例が載っており数えればわかるじゃんみたいな問題も出てきていたので、知らないと手も足も出ないという難易度ではなかったと思います。</p>\n<p>とはいえ、IPv4とはお作法が異なる世界なので、事前にある程度雰囲気を掴んでおかないと試験時間だけで何とかするのは厳しいと思います。</p>\n<p>採点講評に『IPv6は普及が進んでいる技術なため、概要をしっかりと理解してほしい。』みたいなことを書かれると思うのと、今後は午後Ⅰに出題され始めると思うので概要だけでも掴んでおくといいと思います。</p>\n<h2>ネットワークスペシャリストは役に立つか</h2>\n<p>ネットワークスペシャリストを持っていると仕事もプライベートも全部うまくいくみたいにはなりませんが、得られた知識は普通に業務で使えますね。</p>\n<p>会社によっては追加のお賃金が貰えたり昇格の条件にもなっていたりするところもあるみたいなので、まぁIT業界から足を洗うとかでなければ取って損は無いのではないでしょうか。</p>\n<p>ちなみに弊社も持っていると中学生のお小遣い程度の報奨金がもらえます。\n勉強に要した時間で時給換算するとまるで最低賃金には届きませんが・・・</p>\n<h2>おわりに</h2>\n<p>IPv6アドレスを何度も手書きさせるのは正直やめてほしいッピ</p>\n<p>おわり</p>",
      "date_published": "2025-07-13T00:00:00Z"
    },
    {
      "id": "https://www.jyuch.dev/posts/2025/07-04-saa-c03/",
      "url": "https://www.jyuch.dev/posts/2025/07-04-saa-c03/",
      "title": "AWS Certified Solutions Architect - Associateに合格したお話",
      "content_html": "<h2>はじめに</h2>\n<p>AWS Certified Solutions Architect - Associateに合格したので、その話についてです。</p>\n<p><img src=\"https://www.jyuch.dev/img/2025/07-04-saa-c03/saa-c03.png\" alt=\"\"></p>\n<h2>モチベーション</h2>\n<p>本っ当にしょうもない理由なのですが、去年のAWS Summit Japanには認定者ラウンジというものが用意されていました。\n文字通りAWSの認定資格を持っている人が使えるというもので、使っている人たちを見て羨ましかったのを今でも覚えています。</p>\n<p>今年のAWS Summit Japanにも参加出来るということで、それならば今年こそは認定者ラウンジを利用したいと思い取得したのが動機となります。</p>\n<p>今年は認定者ラウンジはありませんでした。</p>\n<h2>おわりに</h2>\n<p>認定シールをもらえたのでよしとします。</p>\n<p>おわり</p>",
      "date_published": "2025-07-04T00:00:00Z"
    }
  ]
}